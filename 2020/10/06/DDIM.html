<!DOCTYPE html>
<!-- _layouts/paper-note.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>DDIM | Molin  Zhang</title>
    <meta name="author" content="Molin  Zhang">
    <meta name="description" content="Denoising Diffusion Implicit Models">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://molinzhang.github.io/2020/10/06/DDIM.html">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <style type="text/css">
      .highlight pre:not(.language-text) { background-color: #272822; color: #f8f8f2;}
      .highlight .hll { background-color: #272822; }
      .highlight .comment { color: #75715e } /* Comment c */
      .highlight .err { color: #960050; background-color: #1e0010 } /* Error */
      .highlight .keyword { color: #66d9ef } /* Keyword k*/
      .highlight .l { color: #ae81ff } /* Literal */
      .highlight .n { color: #f8f8f2 } /* Name */
      .highlight .operator { color: #f92672 } /* Operator o*/
      .highlight .punctuation { color: #f8f8f2 } /* Punctuation p*/
      .highlight .cm { color: #75715e } /* Comment.Multiline */
      .highlight .cp { color: #75715e } /* Comment.Preproc */
      .highlight .c1 { color: #75715e } /* Comment.Single */
      .highlight .cs { color: #75715e } /* Comment.Special */
      .highlight .ge { font-style: italic } /* Generic.Emph */
      .highlight .gs { font-weight: bold } /* Generic.Strong */
      .highlight .kc { color: #66d9ef } /* Keyword.Constant */
      .highlight .kd { color: #66d9ef } /* Keyword.Declaration */
      .highlight .kn { color: #f92672 } /* Keyword.Namespace */
      .highlight .kp { color: #66d9ef } /* Keyword.Pseudo */
      .highlight .kr { color: #66d9ef } /* Keyword.Reserved */
      .highlight .kt { color: #66d9ef } /* Keyword.Type */
      .highlight .ld { color: #e6db74 } /* Literal.Date */
      .highlight .number { color: #ae81ff } /* Literal.Number m*/
      .highlight .string { color: #e6db74 } /* Literal.String s*/
      .highlight .na { color: #a6e22e } /* Name.Attribute */
      .highlight .builtin { color: #f8f8f2 } /* Name.Builtin nb*/
      .highlight .class-name { color: #a6e22e } /* Name.Class nc*/
      .highlight .no { color: #66d9ef } /* Name.Constant */
      .highlight .decorator { color: #a6e22e } /* Name.Decorator nd*/
      .highlight .ni { color: #f8f8f2 } /* Name.Entity */
      .highlight .ne { color: #a6e22e } /* Name.Exception */
      .highlight .function { color: #a6e22e } /* Name.Function nf*/
      .highlight .nl { color: #f8f8f2 } /* Name.Label */
      .highlight .nn { color: #f8f8f2 } /* Name.Namespace */
      .highlight .nx { color: #a6e22e } /* Name.Other */
      .highlight .py { color: #f8f8f2 } /* Name.Property */
      .highlight .nt { color: #f92672 } /* Name.Tag */
      .highlight .nv { color: #f8f8f2 } /* Name.Variable */
      .highlight .ow { color: #f92672 } /* Operator.Word */
      .highlight .w { color: #f8f8f2 } /* Text.Whitespace */
      .highlight .mf { color: #ae81ff } /* Literal.Number.Float */
      .highlight .mh { color: #ae81ff } /* Literal.Number.Hex */
      .highlight .mi { color: #ae81ff } /* Literal.Number.Integer */
      .highlight .mo { color: #ae81ff } /* Literal.Number.Oct */
      .highlight .sb { color: #e6db74 } /* Literal.String.Backtick */
      .highlight .sc { color: #e6db74 } /* Literal.String.Char */
      .highlight .sd { color: #e6db74 } /* Literal.String.Doc */
      .highlight .s2 { color: #e6db74 } /* Literal.String.Double */
      .highlight .se { color: #ae81ff } /* Literal.String.Escape */
      .highlight .sh { color: #e6db74 } /* Literal.String.Heredoc */
      .highlight .si { color: #e6db74 } /* Literal.String.Interpol */
      .highlight .sx { color: #e6db74 } /* Literal.String.Other */
      .highlight .sr { color: #e6db74 } /* Literal.String.Regex */
      .highlight .s1 { color: #e6db74 } /* Literal.String.Single */
      .highlight .ss { color: #e6db74 } /* Literal.String.Symbol */
      .highlight .bp { color: #f8f8f2 } /* Name.Builtin.Pseudo */
      .highlight .vc { color: #f8f8f2 } /* Name.Variable.Class */
      .highlight .vg { color: #f8f8f2 } /* Name.Variable.Global */
      .highlight .vi { color: #f8f8f2 } /* Name.Variable.Instance */
      .highlight .il { color: #ae81ff } /* Literal.Number.Integer.Long */
      .highlight .gh { } /* Generic Heading & Diff Header */
      .highlight .gu { color: #75715e; } /* Generic.Subheading & Diff Unified/Comment? */
      .highlight .gd { color: #f92672; } /* Generic.Deleted & Diff Deleted */
      .highlight .gi { color: #a6e22e; } /* Generic.Inserted & Diff Inserted */
    </style>
    <script> configObj = { "buttonD": "M8 18.568L10.8 21.333 16 16.198 21.2 21.333 24 18.568 16 10.667z", "buttonT": "translate(-1148 -172) translate(832 140) translate(32 32) translate(284)", "shadowSize": "none", "roundnessSize": "999px", "buttonDToBottom": "64px", "buttonDToRight": "32px", "selectedBackgroundColor": "#c2c0bf", "selectedIconColor": "#a31f34", "buttonWidth": "40px", "buttonHeight": "40px", "svgWidth": "32px", "svgHeight": "32px" }; function createButton(obj, pageSimulator) { const body = document.querySelector("body"); backToTopButton = document.createElement("span"); backToTopButton.classList.add("softr-back-to-top-button"); backToTopButton.id = "softr-back-to-top-button"; pageSimulator ? pageSimulator.appendChild(backToTopButton) : body.appendChild(backToTopButton); backToTopButton.style.width = obj.buttonWidth; backToTopButton.style.height = obj.buttonHeight; backToTopButton.style.marginRight = obj.buttonDToRight; backToTopButton.style.marginBottom = obj.buttonDToBottom; backToTopButton.style.borderRadius = obj.roundnessSize; backToTopButton.style.boxShadow = obj.shadowSize; backToTopButton.style.color = obj.selectedBackgroundColor; backToTopButton.style.backgroundColor = obj.selectedBackgroundColor; pageSimulator ? backToTopButton.style.position = "absolute" : backToTopButton.style.position = "fixed"; backToTopButton.style.outline = "none"; backToTopButton.style.bottom = "0px"; backToTopButton.style.right = "0px"; backToTopButton.style.cursor = "pointer"; backToTopButton.style.textAlign = "center"; backToTopButton.style.border = "solid 2px currentColor"; backToTopButton.innerHTML = '<svg class="back-to-top-button-svg" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32" > <g fill="none" fill-rule="evenodd"> <path d="M0 0H32V32H0z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> <path class="back-to-top-button-img" fill-rule="nonzero" d="M11.384 13.333h9.232c.638 0 .958.68.505 1.079l-4.613 4.07c-.28.246-.736.246-1.016 0l-4.613-4.07c-.453-.399-.133-1.079.505-1.079z" transform="translate(-1028 -172) translate(832 140) translate(32 32) translate(164) matrix(1 0 0 -1 0 32)" /> </g> </svg>'; backToTopButtonSvg = document.querySelector(".back-to-top-button-svg"); backToTopButtonSvg.style.verticalAlign = "middle"; backToTopButtonSvg.style.margin = "auto"; backToTopButtonSvg.style.justifyContent = "center"; backToTopButtonSvg.style.width = obj.svgWidth; backToTopButtonSvg.style.height = obj.svgHeight; backToTopButton.appendChild(backToTopButtonSvg); backToTopButtonImg = document.querySelector(".back-to-top-button-img"); backToTopButtonImg.style.fill = obj.selectedIconColor; backToTopButtonSvg.appendChild(backToTopButtonImg); backToTopButtonImg.setAttribute("d", obj.buttonD); backToTopButtonImg.setAttribute("transform", obj.buttonT); if (!pageSimulator) { backToTopButton.style.display = "none"; window.onscroll = function () { if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) { backToTopButton.style.display = "block"; } else { backToTopButton.style.display = "none"; } }; backToTopButton.onclick = function () { document.body.scrollTop = 0; document.documentElement.scrollTop = 0; }; } }; document.addEventListener("DOMContentLoaded", function () { createButton(configObj, null); });</script>
  </head>
  
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Molin </span>Zhang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/services/">services</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>DDIM</h1>
        <p>Denoising Diffusion Implicit Models</p>
      </d-title>

      <d-byline>
          <div class="byline grid">
            <div>
              <h3>Published</h3>
                <p>October 6, 2020</p> 
            </div>
            
            <div>
              <h3>Paper</h3>
                <p><a href="https://arxiv.org/pdf/2010.02502.pdf" rel="external nofollow noopener" target="_blank">arXiv</a></p> 
            </div>
            
            
            <div>
              <h3>Code</h3>
                <p><a href="https://github.com/ermongroup/ddim" rel="external nofollow noopener" target="_blank">Github</a></p> 
            </div>
            
          </div>
      </d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#takeaways">Takeaways</a></div>
            <div><a href="#introduction">Introduction</a></div>
            <div><a href="#methods">Methods</a></div>
            <ul>
              <li><a href="#non-markovian-forward-processes">Non-Markovian Forward Processes</a></li>
              <li><a href="#generative-process">Generative Process</a></li>
              <li><a href="#unified-variational-inference-objective">Unified Variational Inference Objective</a></li>
              <li><a href="#sampling-from-generalized-generative-processes">Sampling from Generalized Generative Processes</a></li>
              
            </ul>
<div><a href="#experiments">Experiments</a></div>
            <ul>
              <li><a href="#sample-quality-and-efficiency">Sample Quality and Efficiency</a></li>
              <li><a href="#sample-consistency-in-ddims">Sample Consistency in DDIMs</a></li>
              <li><a href="#interpolation-in-deterministic-generative-processes">Interpolation in Deterministic Generative Processes</a></li>
              <li><a href="#reconstruction-from-latent-space">Reconstruction from Latent Space</a></li>
              
            </ul>
<div><a href="#appendix">Appendix</a></div>
            
          </nav>
        </d-contents>

        <h2 id="takeaways">Takeaways</h2>

<ul>
  <li>This work generalizes DDPMs<d-cite key="DDPM"></d-cite> via a class of <strong>non-Markovian</strong> diffusion processes that lead to the same training objective.</li>
  <li>These non-Markovian processes interpolate between the original DDPMs and implicit models (DDIM) that have deterministic generative processes.</li>
  <li>With the same training procedure as DDPMs, this work provides a more efficient way for sampling by only considering a subsequence of latent variables.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Problems with DDPMs:</p>

<ul>
  <li>DDPMs require many iterations (~1000, the same as the number of forward steps) to produce a high-quality sample.</li>
  <li>The DDPM objective only depends on the “marginals” \(q(x_t\vert x_0)\), but not directly on the “joint” \(q(x_{1:T}\vert x_0)\). There are many inference distributions (joints) with the same marginals</li>
</ul>

<p>Ideas:</p>

<ul>
  <li>Explore non-Markovian inference processes, for which we are still able to design suitable reverse generative Markov chains.</li>
  <li>Show that the resulting variational training objectives have a shared surrogate objective, which is exactly the objective used to train DDPM.</li>
  <li>The shared objective allows us to choose from a large family of generative models using the same neural network simply by choosing a different, non-Markovian diffusion process.</li>
  <li>We are able to use non-Markovian diffusion processes which lead to “short” generative Markov chains that can be simulated in a small number of steps.</li>
</ul>

<h2 id="methods">Methods</h2>

<h3 id="non-markovian-forward-processes">Non-Markovian Forward Processes</h3>

<p>Define a family \(\mathcal{Q}\) of (inference) distributions, indexed by vector \(\sigma\in\mathbb{R}^T_{\ge0}\):</p>

\[q_\sigma(x_{1:T}\vert x_0):=q_\sigma(x_T\vert x_0)\prod_{t=2}^T q_\sigma(x_{t-1}\vert x_t,x_0)\]

<p>where</p>

\[\begin{aligned}
q_\sigma(x_T\vert x_0)&amp;=\mathcal{N}(\sqrt{\alpha_T}x_0,(1-\alpha_T)I),\\
q_\sigma(x_{t-1}\vert x_t, x_0)&amp;=\mathcal{N}(\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}},\sigma_t^2I)\quad\text{for all } t&gt;1.
\end{aligned}\]

<p>Remarks:</p>

<ol>
  <li>The mean is chosen so that \(q_\sigma(x_t\vert x_0)=\mathcal{N}(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\) (see <a href="#the-inference-distribution">Appendix</a>).</li>
  <li>The joint is factorized in reverse order.</li>
  <li>The forward process \(q_\sigma(x_t\vert x_{t-1}, x_0)\) can be derived from Bayes’rule, which is also Gaussian, but is non-Markovian in comparison to DDPM<d-cite key="DDPM"></d-cite>.</li>
  <li>The variance \(\sigma_t^2\) of \(q_\sigma(x_{t-1}\vert x_t, x_0)\) is a hyperparameter that can be choosen. In contrast, the variance of \(q(x_{t-1}\vert x_t, x_0)\) in DDPM is determined by \(\alpha\) due to the Markovian model.</li>
</ol>

<h3 id="generative-process">Generative Process</h3>

<p>Define a trainable generative process</p>

\[p_\theta(x_{0:T}):=p_\theta(x_T)\prod_{i=1}^Tp_\theta^{(t)}(x_{t-1}\vert x_t),\]

<p>where each \(p_\theta^{(t)}(x_{t-1}\vert x_t)\) leverages knowledge of \(q_\sigma(x_{t-1}\vert x_t, x_0)\).</p>

<ol>
  <li>Given a noisy observation \(X_t\), e.g., \(X_t=\sqrt{\alpha_t}X_0+\sqrt{1-\alpha_t}\epsilon_t\) with \(X_0\sim q(x_0)\) and \(\epsilon_t\sim\mathcal{N}(0,I)\).</li>
  <li>
    <p>Make a prediction of the corresponding \(X_0\): The model \(\epsilon_\theta^{(t)}(x_t)\) predicts \(\epsilon_t\) from \(X_t\), without knowing \(X_0\). Then we can predict the <em>denoised observation</em>, which is a prediction of \(X_0\) given \(X_t\),</p>

\[f_\theta^{(t)}(x_t):=\frac{1}{\sqrt{\alpha_t}}(x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)).\]
  </li>
  <li>
    <p>Use the prediction to obtain to sample \(X_{t-1}\) from the reverse conditional distribution \(q_\sigma(x_{t-1}\vert x_t,x_0)\): we can define the generative process with a fixed prior \(p_\theta(x_T)=\mathcal{N}(0,I)\) and</p>

\[p_\theta^{(t)}(x_{t-1}\vert x_t)=\left\{
 \begin{aligned}
 &amp;\mathcal{N}(f_\theta^{(1)}(x_1),\sigma_1^2 I)\quad&amp;\text{if }t=1,\\
 &amp;q_\sigma(x_{t-1}\vert x_t,f_\theta^{(t)}(x_t))\quad&amp;\text{if }t\ge1,
 \end{aligned}
 \right.\]

    <p>where Gaussian noise is added to the case of \(t=1\) to ensure that the generative process is supported everywhere.</p>
  </li>
</ol>

<p>Remarks: This generative process is basically the same as DDPM with some minor differences</p>

<ul>
  <li>This work uses \(q_\sigma(x_{t-1}\vert x_t,x_0)\) while DDPM uses \(q(x_{t-1}\vert x_t, x_0)\).</li>
  <li>This work uses the same variance \(\sigma_t^2\) for \(q_\sigma(x_{t-1}\vert x_t,x_0)\) and \(p_\theta^{(t)}(x_{t-1}\vert x_t)\) while DDPM introduce \(\sigma_t^2\) in \(p_\theta^{(t)}(x_{t-1}\vert x_t)\), which might be different from the \(\tilde\beta_t\) in \(q(x_{t-1}\vert x_t, x_0)\).</li>
</ul>

<h3 id="unified-variational-inference-objective">Unified Variational Inference Objective</h3>

<p>The parameters \(\theta\) are optimized via the variational inference objective</p>

\[J_\sigma(\epsilon_\theta):=\mathbb{E}_{q_\sigma}[\log q_\sigma(X_{1:T}\vert X_0)-\log p_\theta(X_{0:T})]\]

<p>In comparison, DDPM optimizes the following objective:</p>

\[L_\gamma(\epsilon_\theta):=\sum_{t=1}^T\gamma_t \mathbb{E}_{X_0,\epsilon_t}\left[\|\epsilon_\theta^{(t)}(\sqrt{\alpha_t}X_0+\sqrt{1-\alpha_t}\epsilon_t)-\epsilon_t\|_2^2\right],\]

<p>where \(\gamma\in\mathbb{R}^T_{&gt;0}\) is a vector of positive coefficients in the objective that depends on \(\alpha_{1:T}\). In DDPM The objective with \(\gamma=1\) is optimized instead to maximize the generation performance of the trained model.</p>

<p><strong>Theorem 1.</strong> For all \(\sigma\in\mathbb{R}^T_{&gt;0}\), there exists \(\gamma\in\mathbb{R}^T_{&gt;0}\) and \(C\in\mathbb{R}\), such that \(J_\sigma=L_\gamma+C\). (see <a href="#proof-of-theorem-1">Appendix</a> for the proof)</p>

<p>Discussion:</p>

<ul>
  <li>If parameters \(\theta\) are not shared across different \(t\), the optimal solution to \(L_\gamma\) will not depend on the weights \(\gamma\) as  global
optimum is achieved by separately maximizing each term in the sum</li>
  <li>This property justified the use of \(L_1\) (i.e., \(\gamma=1\)) as a surrogate objective function for the variational lower bound in DDPMs.</li>
  <li>Since \(J_\sigma\) is equivalent to some \(L_\gamma\) from Theorem 1, the optimal solution of \(J\sigma\) is also the same as that of \(L_1\).</li>
  <li>Therefore, if parameters are not shared across \(t\), then the \(L_1\) objective used by DDPMs can be used as a surrogate objective for the variational objective \(J_\sigma\) as well.</li>
</ul>

<h3 id="sampling-from-generalized-generative-processes">Sampling from Generalized Generative Processes</h3>

<p>With \(L_1\) as the objective (\(\sigma\) does not appear in the loss), we are not only learning a generative process for the Markovian inference process considered DDPM, but also generative processes for many non-Markovian forward processes parametrized by \(\sigma\) that described above.</p>

<p>Use pre-trained DDPM models as the solutions to the new objectives, and focus on finding a generative process that is better at producing samples subject to our needs by changing \(\sigma\).</p>

<h4 id="denoising-diffusion-implicit-models">Denoising Diffusion Implicit Models</h4>

<p>Generate a sample \(x_{t-1}\) from a sample \(x_t\):</p>

\[x_{t-1}=
\underbrace{\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)}_{\text{predicted }x_0}
+
\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta^{(t)}(x_t)}_{\text{direction pointing to }x_t}
+
\underbrace{\sigma_t\epsilon_t}_\text{random noise}\]

<p>where \(\epsilon_t\sim\mathcal{N}(0, I)\). Different choices of \(\sigma\) result in different generative processes, all while using the same model \(\epsilon_\theta\), so re-training the model is unnecessary.</p>

<ul>
  <li>DDPM
    <ul>
      <li>Set \(\sigma_t=\sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{1-\alpha_t/\alpha_{t-1}}\).</li>
      <li>The forward process becomes Markovian.</li>
    </ul>
  </li>
  <li>DDIM (denoising diffusion implict model)
    <ul>
      <li>Set \(\sigma_t=0\) for all \(t\).</li>
      <li>The forward process becomes deterministic given \(x_{t-1}\) and \(x_{0}\).</li>
      <li>Samples are generated from latent variables with a fixed procedure (from \(x_T\) to \(x_0\)).</li>
    </ul>
  </li>
</ul>

<h4 id="accelerated-generation-processes">Accelerated Generation Processes</h4>

<p>The generative process is considered as the approximation to the reverse
process, and therefore, they should have the same number of time steps \(T\).</p>

<p>However, as \(L_1\) does not depend on the specific forward procedure as long as \(q_\sigma(x_t\vert x_0)\) is fixed, we may also consider forward processes with lengths smaller than \(T\), which accelerates the corresponding generative processes without having to train a different model.</p>

<ol>
  <li>Consider a subset \(\{X_{\tau_1},\dots X_{\tau_S}\}\), where \(\tau\) is an increasing sub-sequence of \([1,\dots, T]\) of length \(S\).</li>
  <li>Define the a forward process over \(X_\tau\) such that \(q(x_{\tau_i}\vert x_0)=\mathcal{N}(\sqrt{\alpha_{\tau_i}}x_0, (1-\alpha_{\tau_i})I)\) matches the “marginals”.</li>
  <li>The generative process now sampled latent variable according to reversed \(\tau\) (<em>sampling trajectory</em>).</li>
</ol>

<p>Details can be found in the <a href="#accelerated-sampling-processes">Appendix</a></p>

<p>Insight:</p>

<ul>
  <li>In principle, we can train a model with an arbitrary number of forward steps but only sample from some of them in the generative process.</li>
  <li>Therefore, the trained model could consider many more steps or even a continuous time variable \(t\)</li>
</ul>

<h4 id="relevance-to-neural-odes">Relevance to Neural ODEs</h4>

<p>The DDIM iterate (i.e., \(\sigma_t=0\)):</p>

\[x_{t-1}=
\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)
+
\sqrt{1-\alpha_{t-1}}\cdot\epsilon_\theta^{(t)}(x_t)\]

<p>can be rewritten as</p>

\[\frac{x_{t-\Delta t}}{\sqrt{\alpha_{t-\Delta t}}}=\frac{x_t}{\sqrt{\alpha_t}}+\left(\sqrt{\frac{1-\alpha_{t-\Delta t}}{\alpha_{t-\Delta t}}}-\sqrt{\frac{1-\alpha_t}{\alpha_t}}\right)\epsilon_\theta^{(t)}(x_t)\]

<p>We can reparameterize \(\sqrt{(1-\alpha)/\alpha}\) with \(\omega\) and \(x/\sqrt{\alpha}\) with \(\bar{x}\). When \(\Delta t\rightarrow 0\), \(\omega\) and \(\bar{x}\) are functions of \(t\), where \(\omega\) is continous, increasing with \(\omega(0)=0\). The above iteration can be treated as an Euler method over the following ODE:</p>

\[\text{d}\bar{x}(t)=\epsilon^{(t)}_\theta\left(\frac{\bar{x}(t)}{\sqrt{\omega^2+1}}\right)\text{d}\omega(t),\]

<p>where the initial conditions is \(\bar{x}(T)=x(T)/\sqrt{\alpha(T)}\sim\mathcal{N}(0,1/\alpha(T))\). Since \(\alpha(T)\approx 0\), The variance \(1/\alpha(T)\) would be very large.</p>

<h2 id="experiments">Experiments</h2>

<p>Key results:</p>

<ul>
  <li>DDIMs outperform DDPMs in terms of image generation <em>when fewer iterations are considered</em>, giving speed-ups of 10x to 100x over the original DDPM generation process.</li>
  <li>Unlike DDPMs, once the initial latent variables \(x_T\) are fixed, DDIMs retain high-level image features regardless of the generation trajectory (different sub-sequences), so they are able to perform interpolation directly from the latent space.</li>
  <li>DDIMs can also be used to encode samples that reconstruct them from the latent code, which DDPMs cannot do due to the stochastic sampling process.</li>
</ul>

<p>Setup:</p>

<ul>
  <li>Use the <strong>same trained model</strong>
    <ul>
      <li>number of time steps \(T=1000\)</li>
      <li>trained with \(L_1\)</li>
    </ul>
  </li>
  <li>
<strong>The only change</strong> is how to produce samples from the model by controlling
    <ul>
      <li>how fast the samples are obtained, \(τ\)</li>
      <li>and sample variance \(\sigma_t^2\), which interpolates between the deterministic DDIM and the stochastic DDPM.
\(\sigma_{\tau_i}(\eta)=\eta\sqrt{\frac{1-\alpha_{\tau_{i-1}}}{1-\alpha_{\tau_i}}}\sqrt{1-\frac{\alpha_{\tau_i}}{\alpha_{\tau_{i-1}}}}\)
where \(\eta&gt;0\) is a hyperparameter. This includes DDPM (\(\eta=1\)), DDIM (\(\eta=0\)), and DDPM with larger variance (denoted as \(\hat{\sigma}:\hat{\sigma}_{\tau_i}=\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}\)).</li>
    </ul>
  </li>
</ul>

<h3 id="sample-quality-and-efficiency">Sample Quality and Efficiency</h3>

<p>Vary the number of timesteps used to generate a sample (\(S=\text{dim}(\tau)\)) and the stochasticity of the process \(\eta\), and present a tradeoff between sample quality and computational costs.</p>

<p>Results:</p>

<ul>
  <li>DDIM (\(\eta=0\)) achieves the best sample quality when \(S\) is small.</li>
  <li>DDPM (\(\eta=1\) and \(\hat{\sigma}\)) typically has worse sample quality compared to its less stochastic counterparts with the same \(S\). (when \(S&lt;T\))</li>
  <li>In the case with \(S=T=1000\), DDPM (\(\hat{\sigma}\)) is better than DDIM.</li>
  <li>The sample quality of DDPM (\(\hat{\sigma}\)) becomes much worse for smaller \(S\), which suggests that it is ill-suited for shorter trajectories.</li>
  <li>DDIM achieves high sample quality much more consistently.</li>
  <li>DDIM is able to produce samples with quality comparable to 1000 step models within 20 to 100 steps.</li>
</ul>

<h3 id="sample-consistency-in-ddims">Sample Consistency in DDIMs</h3>

<p>For DDIM, the generative process is deterministic, and \(x_0\) would depend only on the initial state \(x_T\).</p>

<p>Compare generated images under different generative trajectories (i.e. different \(\tau\)) while starting with the same initial \(x_T\)</p>

<p>Results:</p>

<ul>
  <li>For the generated images with the same initial \(x_T\), most high-level features are similar, regardless of the generative trajectory.</li>
  <li>It indicates that \(x_T\) alone would be an informative latent encoding of the image.</li>
  <li>Minor details that affect sample quality are encoded in the parameters.</li>
</ul>

<h3 id="interpolation-in-deterministic-generative-processes">Interpolation in Deterministic Generative Processes</h3>

<p>Since the high-level features of the DDIM sample are encoded by \(x_T\), it might be used for semantic interpolation.</p>

<p>This is different from the interpolation procedure in DDPM, where the same \(x_T\) would lead to highly diverse \(x_0\) due to the stochastic generative process</p>

<p>DDIM is able to control the generated images on a high level directly through the latent variables, which DDPMs cannot.</p>

<h3 id="reconstruction-from-latent-space">Reconstruction from Latent Space</h3>

<p>As DDIM is the Euler integration for a particular ODE, it should be able to encode from \(x_0\) to \(x_T\) (reverse of the ODE) and reconstruct \(x_0\) from the resulting \(x_T\) (forward of the ODE).</p>

<p>Results: DDIMs have lower reconstruction error for larger \(S\) and have properties similar to Neural ODEs and normalizing flows. The same cannot be said for DDPMs due to their stochastic nature.</p>

<h2 id="appendix">Appendix</h2>

<h3 id="comparision-between-notations-in-ddpm-and-ddim">Comparision between Notations in DDPM and DDIM</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">meaning</th>
      <th style="text-align: center">DDPM<d-cite key="DDPM"></d-cite>
</th>
      <th style="text-align: center">DDIM (this work)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">diffusion rate</td>
      <td style="text-align: center">\(\beta_t\)</td>
      <td style="text-align: center">\(1-\alpha_t/\alpha_{t-1}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">1-diffusion rate</td>
      <td style="text-align: center">\(\alpha_t\)</td>
      <td style="text-align: center">\(\alpha_t/\alpha_{t-1}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">product of 1-diffusion rate</td>
      <td style="text-align: center">\(\overline{\alpha}_t\)</td>
      <td style="text-align: center">\(\alpha_t\)</td>
    </tr>
  </tbody>
</table>

<h3 id="marginal-and-conditional-gaussians">Marginal and Conditional Gaussians</h3>

<p>The materials in this section are from Pattern Recognition and Machine Learning (Bishop, 2006)  Section 2.3.3.</p>

<p>Given a marginal Gaussian distribution for \(x\) and a conditional Gaussian distribution for \(y\) given \(x\) in the form</p>

\[\begin{aligned}
p(x)&amp;=\mathcal{N}(x;\mu,\Lambda^{-1})\\
p(y\vert x)&amp;=\mathcal{N}(y; Ax+b,L^{-1}) \\
\end{aligned}\]

<p>The marginal distribution of \(y\) and the conditional distribution of \(x\) given \(y\) are given by</p>

\[\begin{aligned}
p(y) &amp;= \mathcal{N}(y; A\mu+b, L^{-1}+A\Lambda^{-1}A^T)\\
p(x|y) &amp;= \mathcal{N}(x; \Sigma\{A^TL(y-b)+\Lambda\mu\}, \Sigma)
\end{aligned}\]

<p>where \(\Sigma=(\Lambda + A^TLA)^{-1}\).</p>

<h3 id="the-inference-distribution">The Inference Distribution</h3>

<p>The core of the inference distribution \(q_\sigma\) is the conditional distribution of \(X_{t-1}\) given \(X_t\) and \(X_0\), i.e.,</p>

\[q_\sigma(x_{t-1}\vert x_t, x_0)=\mathcal{N}(\tilde{\mu}_t(x_t,x_0),\sigma_t^2I),\]

<p>where \(\tilde\mu_t\) is the mean function. Assuming it takes a linear form, i.e., \(\tilde\mu_t(x_t, x_0)=ax_t+bx_0\), where \(a\) and \(b\) are constants to be determined. We want the proposed joint distribution to match the “marginals” of the original DM. Specifically, suppose \(q_\sigma(x_t\vert x_0)=\mathcal{N}(\sqrt{\alpha_t} x_0, (1-\alpha_t)I)\), we want \(q_\sigma(x_{t-1}\vert x_0)=\mathcal{N}(\sqrt{\alpha_{t-1}} x_0, (1-\alpha_{t-1})I)\). We can compute \(q_\sigma(x_{t-1}\vert x_0)\) from \(q_\sigma(x_{t}\vert x_0)\) and \(q_\sigma(x_{t-1}\vert x_t,x_0)\) as follows. (see <a href="#marginal-and-conditional-gaussians">this section</a>)</p>

\[q_\sigma(x_{t-1}\vert x_0)=\mathcal{N}(a\sqrt{\alpha_t}x_0+bx_0,[\sigma_t^2+(1-\alpha_{t})a^2]I)\]

<p>We solve the following equations to match the mean and the variance:
\(\begin{aligned}
a\sqrt{\alpha_t}+b&amp;=\sqrt{\alpha_{t-1}}\\
\sigma_t^2+(1-\alpha_{t})a^2&amp;=1-\alpha_{t-1}
\end{aligned}\)</p>

<p>which givens</p>

\[\begin{aligned}
a&amp;=\frac{\sqrt{1-\alpha_{t-1}-\sigma_t^2}}{\sqrt{1-\alpha_{t}}}\\
b&amp;=\sqrt{\alpha_{t-1}}-\frac{\sqrt{\alpha_t}\sqrt{1-\alpha_{t-1}-\sigma_t^2}}{\sqrt{1-\alpha_{t}}}
\end{aligned}\]

<p>Therefore,</p>

\[\tilde\mu_t=a x_t+bx_0=\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_{t}}}\]

<p>and</p>

\[q_\sigma(x_{t-1}\vert x_t, x_0)=\mathcal{N}(\sqrt{\alpha_{t-1}}x_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}},\sigma_t^2I)\]

<p>In comparison, DDPM uses different mean and variance for \(q(x_{t-1}\vert x_t, x_0)\):</p>

\[\begin{aligned}
q(x_{t-1}\vert x_t, x_0)&amp;=\mathcal{N}\left(\frac{\sqrt{\alpha_{t-1}}}{1-\alpha_{t}}\left(1-\frac{\alpha_t}{\alpha_{t-1}}\right)x_0 + \frac{\sqrt{\alpha_t}(1-\alpha_{t-1})}{\sqrt{\alpha_{t-1}}(1-\alpha_t)}x_t, \frac{1-\alpha_{t-1}}{1-\alpha_t}(1-\frac{\alpha_t}{\alpha_{t-1}})I\right) \\
&amp;=\mathcal{N}\left(\sqrt{\alpha_{t-1}}x_0+\frac{\sqrt{\alpha_t}(1-\alpha_{t-1})}{\sqrt{\alpha_{t-1}}\sqrt{1-\alpha_t}}\cdot\frac{x_t-\sqrt{\alpha_t}x_0}{\sqrt{1-\alpha_t}},\frac{1-\alpha_{t-1}}{1-\alpha_t}(1-\frac{\alpha_t}{\alpha_{t-1}})I \right)
\end{aligned}\]

<p>If we set</p>

\[\sigma_t^2=\frac{1-\alpha_{t-1}}{1-\alpha_t}\left(1-\frac{\alpha_t}{\alpha_{t-1}}\right),\]

<p>then \(q_\sigma(x_{t-1}\vert x_t, x_0)=q(x_{t-1}\vert x_t, x_0)\) and the model becomes DDPM.</p>

<h3 id="proof-of-theorem-1">Proof of Theorem 1</h3>

<p>For all \(\sigma\in\mathbb{R}^T_{&gt;0}\), there exists \(\gamma\in\mathbb{R}^T_{&gt;0}\) and \(C\in\mathbb{R}\), such that \(J_\sigma=L_\gamma+C\).</p>

<p>Proof:</p>

<p>Following the derivation of DDPM (where \(\equiv\) denotes “equal up to a value that does not depend on \(\theta\), but may depend on \(q_\sigma\)”).</p>

\[\begin{aligned}
J_\sigma(\epsilon_\theta)&amp;:=\mathbb{E}_{q_\sigma}[\log q_\sigma(X_{1:T}\vert X_0)-\log p_\theta(X_{0:T})] \\
&amp;\equiv\mathbb{E}_{q_\sigma}\left[\sum_{t=2}^T D_\text{KL}(q_\sigma(x_{t-1}|X_t,X_0)||p_\theta^{(t)}(x_{t-1}|X_t)) -\log p_\theta^{1}(X_0|X_1)\right]\\
\end{aligned}\]

<p>For \(t&gt;1\):</p>

\[\begin{aligned}
\mathbb{E}_{q_\sigma}\left[ D_\text{KL}(q_\sigma(x_{t-1}|X_t,X_0)||p_\theta^{(t)}(x_{t-1}|X_t))\right]&amp;=\mathbb{E}_{X_0,X_t}\left[ D_\text{KL}(q_\sigma(x_{t-1}|X_t,X_0)||q_\sigma(x_{t-1}|X_t,f_\theta^{t}(X_t)))\right]\\
&amp;\equiv\mathbb{E}_{X_0,X_t}\left[\frac{\|\tilde{\mu}_t(X_t,X_0)-\tilde{\mu}_t(X_t,f_\theta^{(t)}(X_t))\|_2^2}{2\sigma_t^2}\right]\\
&amp;=\mathbb{E}_{X_0,X_t}\left[\frac{b_t^2}{2\sigma_t^2}\|X_0-f_\theta^{(t)}(X_t)\|_2^2\right]\\
&amp;=\mathbb{E}_{X_0,\epsilon}\left[\frac{b_t^2(1-\alpha_t)}{2\sigma_t^2\alpha_t}\|\epsilon-\epsilon_\theta^{(t)}(X_t)\|_2^2\right]\\
\end{aligned}\]

<p>For \(t=1\):</p>

\[\begin{aligned}
\mathbb{E}_{q_\sigma}\left[ -\log p_\theta^{1}(X_0|X_1)\right]&amp;\equiv\mathbb{E}_{X_0,X_t}\left[\frac{1}{2\sigma_t^2}\|X_0-f_\theta^{(t)}(X_1)\|_2^2\right]\\
&amp;=\mathbb{E}_{X_0,\epsilon}\left[\frac{(1-\alpha_t)}{2\sigma_t^2\alpha_t}\|\epsilon-\epsilon_\theta^{(t)}(X_1)\|_2^2\right]\\
\end{aligned}\]

<p>Choosing \(\gamma_1=(1-\alpha_t)/(2\sigma_t^2\alpha_t)\) and \(\gamma_t=(1-\alpha_t)b_t^2/(2\sigma_t^2\alpha_t)\) for \(t&gt;1\), we have \(J_\sigma(\epsilon_\theta)\equiv L_\gamma(\epsilon_\theta)\).</p>

<h3 id="accelerated-sampling-processes">Accelerated Sampling Processes</h3>

<p>The inference process in the accelerated case is given by</p>

\[q_{\sigma,\tau}(x_{1:T}\vert x_0)=q_{\sigma, \tau}(x_{\tau_S}\vert x_0)\prod_{i=2}^S q_{\sigma, \tau}(x_{\tau_{i-1}}\vert x_{\tau_i}, x_0)\prod_{t\in\overline\tau}q_{\sigma, \tau}(x_t|x_0),\]

<p>where \(\tau\) is a sub-sequence of \([1,\dots, T]\) of length \(S\) with \(\tau_S=T\), and \(\overline\tau:=\{1,\dots, T\}\backslash \tau\), i.e., the graphical model of \(\{X_{\tau_i}\}_{i=1}^S\) and \(X_0\) form a chain, whereas the graphical model of \(\{X_t\}_{t\in\overline\tau}\) and \(X_0\) form a star graph.</p>

<p>Define:</p>

\[\begin{aligned}
q_{\sigma,\tau}(x_t\vert x_0)&amp;=\mathcal{N}(\sqrt{\alpha_t}x_0,(1-\alpha_t)I)\quad\forall t\in\overline\tau\cup\{T\}\\
q_{\sigma,\tau}(x_{\tau_{i-1}}\vert x_{\tau_i},x_0)&amp;=\mathcal{N}(\sqrt{\alpha_{\tau_{i-1}}}x_0+\sqrt{1-\alpha_{\tau_{i-1}}-\sigma_{\tau_i}^2}\cdot\frac{x_{\tau_i}-\sqrt{\alpha_{\tau_i}}x_0}{\sqrt{1-\alpha_{\tau_i}}},\sigma_{\tau_i}^2I),\quad 2\le i\le S
\end{aligned}\]

<p>where the coefficients are chosen such that:</p>

\[q_{\sigma,\tau}(x_{\tau_i}|x_0)=\mathcal{N}(\sqrt{\alpha_{\tau_i}}x_0,(1-\alpha_{\tau_i})I)\quad 1\le i\le S,\]

<p>i.e., the “marginals” match.</p>

<p>The corresponding “generative process” is defined as:</p>

\[p_\theta(x_{0:T}):=
\underbrace{p_\theta(x_T)\prod_{i=1}^Sp_{\theta}^{(\tau_i)}(x_{\tau_{i-1}}\vert x_{\tau_i})}_\text{use to produce samples}
\times
\underbrace{\prod_{t\in\overline\tau}p_\theta^{(t)}(x_0\vert x_t)}_\text{use in objective},\]

<p>where only part of the models are actually being used to produce samples (define \(\tau_0=0\)). The conditionals are:</p>

\[\begin{aligned}
p_\theta^{\tau_{i}}(x_{\tau_{i-1}}\vert x_{\tau_i})&amp;=q_{\sigma,\tau}(x_{\tau_{i-1}}\vert x_{\tau_i}, f_\theta^{(\tau_i)}(x_{\tau_i}))\quad\text{if }i\in\{2,\dots, S\}\\
p_\theta^{(t)}(x_0\vert x_t)&amp;=\mathcal{N}(f_\theta^{(t)}(x_{t}),\sigma_{t}^2I),\quad\text{if }t\in\overline\tau\cup\{\tau_1\},
\end{aligned}\]

<p>which leverages \(q_{\sigma,\tau}(x_{\tau_{i-1}}\vert x_{\tau_i}, x_0)\) as part of the inference process.</p>

<p>The resulting variational objective becomes (define \(x_{\tau_{L+}}\))</p>

\[\begin{aligned}
J_{\sigma,\tau}(\epsilon_\theta)&amp;=\mathbb{E}_{q_{\sigma,\tau}}[\log q_{\sigma,\tau}(X_{1:T}\vert X_0)-\log p_\theta(X_{0:T})]\\
&amp;\equiv\mathbb{E}_{q_{\sigma,\tau}}\left[\sum_{i=2}^S D_\text{KL}(q_{\sigma,\tau}(x_{\tau_{i-1}}|X_{\tau_i},X_0)||p_\theta^{(\tau_i)}(x_{\tau_{i-1}}|X_{\tau_i})) -\log p_\theta^{(\tau_1)}(X_0|X_{\tau_i})\right.\\
&amp;\qquad+\left. \sum_{t\in\overline\tau} -\log p_\theta^{(t)}(X_0|X_{t}) \right]\\

\end{aligned}\]

<p>A similar argument to the proof used in <a href="#proof-of-theorem-1">Theorem 1</a> can show that \(J_{\sigma,\tau}\) can also be converted to an objective of the form \(L_\gamma\).</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/paper-notes.bib"></d-bibliography>
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Molin  Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.
Last updated: August 22, 2023.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
